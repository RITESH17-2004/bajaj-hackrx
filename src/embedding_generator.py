from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict
import tiktoken
import asyncio
import functools
from concurrent.futures import ThreadPoolExecutor
import logging

class EmbeddingGenerator:
    """
    Manages the generation of embeddings for text chunks and queries using a SentenceTransformer model.
    Handles tokenization, text truncation, and provides methods for similarity calculation.
    """
    def __init__(self, model_name: str = "paraphrase-MiniLM-L3-v2", device: str = "cpu", executor: ThreadPoolExecutor = None):
        # Load the sentence transformer model and move it to the specified device (CPU/GPU)
        self.model = SentenceTransformer(model_name).to(device)
        # Initialize tokenizer for token counting and truncation
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.max_tokens = 512  # Maximum tokens allowed for input text
        self.executor = executor # Thread pool executor for running blocking operations asynchronously
    
    async def generate_embeddings(self, chunks: List[Dict]) -> np.ndarray:
        """
        Generates embeddings for a list of text chunks.
        Truncates text if it exceeds the maximum token limit.
        """
        logging.info(f"Generating embeddings for {len(chunks)} chunks.")
        texts = []
        for chunk in chunks:
            text = chunk['text']
            # Truncate text if it exceeds the maximum token limit
            if self._count_tokens(text) > self.max_tokens:
                text = self._truncate_text(text)
            texts.append(text)
        
        loop = asyncio.get_event_loop()
        # Run the blocking model encoding operation in the thread pool executor
        embeddings = await loop.run_in_executor(
            self.executor,
            functools.partial(self.model.encode, texts, convert_to_numpy=True)
        )
        logging.info("Embeddings generated successfully.")
        return embeddings
    
    async def generate_query_embedding(self, query: str) -> np.ndarray:
        """
        Generates an embedding for a single query string.
        Truncates query if it exceeds the maximum token limit.
        """
        logging.info(f"Generating query embedding for: {query[:50]}...")
        # Truncate query if it exceeds the maximum token limit
        if self._count_tokens(query) > self.max_tokens:
            query = self._truncate_text(query)
        
        loop = asyncio.get_event_loop()
        # Run the blocking model encoding operation in the thread pool executor
        embedding = await loop.run_in_executor(
            self.executor,
            functools.partial(self.model.encode, [query], convert_to_numpy=True)
        )
        logging.info("Query embedding generated successfully.")
        return embedding[0]
    
    def _count_tokens(self, text: str) -> int:
        """
        Counts the number of tokens in a given text using the initialized tokenizer.
        """
        return len(self.tokenizer.encode(text))
    
    def _truncate_text(self, text: str) -> str:
        """
        Truncates the text to `self.max_tokens` if it exceeds the limit.
        """
        tokens = self.tokenizer.encode(text)
        if len(tokens) <= self.max_tokens:
            return text
        
        truncated_tokens = tokens[:self.max_tokens]
        return self.tokenizer.decode(truncated_tokens)
    
    def calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        Calculates the cosine similarity between two embeddings.
        """
        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
    
    def get_embedding_dimension(self) -> int:
        """
        Returns the dimension of the embeddings generated by the model.
        """
        return self.model.get_sentence_embedding_dimension()
